# SOLUTION ARCHITECTURE: HLTH Outreach CRM
## Version 1.0 - MVP for Single User (On-Prem)

---

## SYSTEM OVERVIEW

```
┌─────────────────────────────────────────────────────────────────┐
│                        INPUT LAYER                               │
│  CSV File with Company Names → Python CLI Interface             │
└─────────────────────┬───────────────────────────────────────────┘
                      │
                      ▼
┌─────────────────────────────────────────────────────────────────┐
│                   ENRICHMENT ENGINE                              │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐         │
│  │ Apollo.io API│  │ Web Scraper  │  │ SerpAPI      │         │
│  │ (Company +   │  │ (News +      │  │ (LinkedIn    │         │
│  │  Contacts)   │  │  Descriptions)│  │  URLs)       │         │
│  └──────────────┘  └──────────────┘  └──────────────┘         │
└─────────────────────┬───────────────────────────────────────────┘
                      │
                      ▼
┌─────────────────────────────────────────────────────────────────┐
│                   PROCESSING LAYER                               │
│  • Data Validation                                               │
│  • Deduplication                                                 │
│  • Tier Assignment (Auto)                                        │
│  • Priority Scoring (1-10)                                       │
│  • Contact Ranking                                               │
└─────────────────────┬───────────────────────────────────────────┘
                      │
                      ▼
┌─────────────────────────────────────────────────────────────────┐
│                    STORAGE LAYER                                 │
│  ┌──────────────────┐         ┌──────────────────┐             │
│  │  SQLite Cache    │◄────────►│  Notion Database │             │
│  │  (Local speed)   │  Sync   │  (Source of Truth)│             │
│  └──────────────────┘         └──────────────────┘             │
└─────────────────────────────────────────────────────────────────┘
```

---

## TECH STACK SPECIFICATION

### Core Dependencies
```txt
# requirements.txt

# Python Version: 3.11+

# Notion Integration
notion-client==2.2.1

# Apollo.io Integration
apollo-python==0.1.0  # Or direct REST API calls

# Web Scraping
requests==2.31.0
beautifulsoup4==4.12.2
lxml==4.9.3

# Search APIs
google-search-results==2.4.2  # SerpAPI

# Data Processing
pandas==2.1.0
python-dotenv==1.0.0

# Rate Limiting & Retry
tenacity==8.2.3
ratelimit==2.2.1

# Utilities
click==8.1.7  # CLI interface
rich==13.5.2  # Pretty terminal output
tqdm==4.66.1  # Progress bars

# Validation
validators==0.22.0
pydantic==2.4.0

# Local Storage
sqlalchemy==2.0.21
```

### System Requirements
- **OS**: macOS/Linux (Windows WSL2 works)
- **Python**: 3.11+
- **RAM**: 2GB minimum
- **Disk**: 500MB for dependencies + data
- **Internet**: Required (API calls)

---

## PROJECT STRUCTURE

```
outreach-crm/
├── .env                          # API keys & config (git-ignored)
├── .gitignore
├── requirements.txt
├── README.md
│
├── config/
│   ├── __init__.py
│   ├── settings.py               # Configuration management
│   └── constants.py              # Static mappings (titles, tiers)
│
├── database/
│   ├── __init__.py
│   ├── models.py                 # SQLAlchemy models
│   ├── notion_schema.py          # Notion DB structure
│   └── cache.db                  # SQLite file (auto-created)
│
├── enrichment/
│   ├── __init__.py
│   ├── apollo_client.py          # Apollo.io integration
│   ├── web_scraper.py            # Website scraping
│   ├── news_scraper.py           # Google News
│   ├── linkedin_finder.py        # SerpAPI LinkedIn search
│   └── enrichment_engine.py      # Main orchestrator
│
├── processors/
│   ├── __init__.py
│   ├── validator.py              # Data validation
│   ├── deduplicator.py           # Duplicate detection
│   ├── tier_assigner.py          # Auto tier assignment
│   ├── priority_scorer.py        # Priority calculation
│   └── contact_ranker.py         # Contact prioritization
│
├── integrations/
│   ├── __init__.py
│   ├── notion_client.py          # Notion API wrapper
│   └── cache_manager.py          # SQLite operations
│
├── utils/
│   ├── __init__.py
│   ├── logger.py                 # Logging setup
│   ├── rate_limiter.py           # API rate limiting
│   └── helpers.py                # Common utilities
│
├── templates/
│   └── notion_page_template.json # Notion page structure
│
├── main.py                        # CLI entry point
└── tests/
    ├── test_enrichment.py
    ├── test_processors.py
    └── test_integration.py
```

---

## DATABASE SCHEMA

### SQLite Cache (Local Speed Layer)

```sql
-- companies table
CREATE TABLE companies (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    company_name TEXT NOT NULL UNIQUE,
    domain TEXT,
    industry TEXT,
    size_range TEXT,
    location TEXT,
    revenue_range TEXT,
    description TEXT,
    linkedin_url TEXT,
    tech_stack TEXT,  -- JSON array as string
    funding_stage TEXT,
    recent_news TEXT,  -- JSON array as string
    apollo_id TEXT,
    enriched_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    notion_page_id TEXT,
    sync_status TEXT DEFAULT 'pending',  -- pending, synced, failed
    UNIQUE(company_name)
);

-- contacts table
CREATE TABLE contacts (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    company_id INTEGER NOT NULL,
    full_name TEXT NOT NULL,
    first_name TEXT,
    last_name TEXT,
    title TEXT,
    seniority TEXT,
    email TEXT,
    phone TEXT,
    linkedin_url TEXT,
    priority_score INTEGER,  -- 1-10
    apollo_id TEXT,
    enriched_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (company_id) REFERENCES companies(id),
    UNIQUE(company_id, email)
);

-- enrichment_log table (for debugging/retry)
CREATE TABLE enrichment_log (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    company_name TEXT NOT NULL,
    step TEXT,  -- 'apollo', 'scraping', 'news', 'linkedin', 'notion_sync'
    status TEXT,  -- 'success', 'failed', 'skipped'
    error_message TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- api_usage table (track credits)
CREATE TABLE api_usage (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    service TEXT,  -- 'apollo', 'serpapi', 'hunter'
    operation TEXT,  -- 'company_search', 'email_reveal', etc.
    credits_used INTEGER,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Notion Database Properties

```python
# database/notion_schema.py

NOTION_PROPERTIES = {
    # Core fields
    "Company Name": {"type": "title"},
    "Status": {
        "type": "select",
        "options": [
            "Not Contacted", "Email Sent", "Opened", "Replied",
            "Meeting Booked", "Meeting Done", "Pilot Discussion",
            "Pilot Signed", "Closed-Lost", "Nurture"
        ]
    },
    "Industry": {
        "type": "select",
        "options": [
            "Insurance / Payer", "Healthcare Provider / Health System",
            "Pharmacy / PBM", "Pharma / Biotech", "Other"
        ]
    },
    "Tier": {
        "type": "select",
        "options": [
            "Tier 1 - AEP Urgent", "Tier 2 - Strategic",
            "Tier 3 - Proven Vertical", "Tier 4 - Exploratory"
        ]
    },
    
    # Enriched company data
    "Company Website": {"type": "url"},
    "Company LinkedIn": {"type": "url"},
    "Company Description": {"type": "rich_text"},
    "Company Size": {
        "type": "select",
        "options": ["1-50", "51-200", "201-1000", "1000-5000", "5000+", "Unknown"]
    },
    "Location": {"type": "rich_text"},
    "Revenue Range": {
        "type": "select",
        "options": ["<$1M", "$1-10M", "$10-50M", "$50-200M", "$200M+", "Unknown"]
    },
    "Tech Stack": {"type": "multi_select", "options": []},  # Dynamic
    "Funding Stage": {
        "type": "select",
        "options": ["Seed", "Series A", "Series B", "Series C+", "Public", "Private", "Unknown"]
    },
    "Recent News": {"type": "rich_text"},
    
    # Contact fields (top 3 contacts as properties)
    "Primary Contact Name": {"type": "rich_text"},
    "Primary Contact Title": {"type": "rich_text"},
    "Primary Contact Email": {"type": "email"},
    "Primary Contact LinkedIn": {"type": "url"},
    
    "Secondary Contact Name": {"type": "rich_text"},
    "Secondary Contact Title": {"type": "rich_text"},
    "Secondary Contact Email": {"type": "email"},
    
    "Tertiary Contact Name": {"type": "rich_text"},
    "Tertiary Contact Title": {"type": "rich_text"},
    "Tertiary Contact Email": {"type": "email"},
    
    # Outreach tracking
    "Priority Score": {"type": "number"},
    "Decision Timeline": {
        "type": "select",
        "options": ["This Week", "This Month", "Q1 2026", "Q2 2026", "Unknown"]
    },
    "Deal Size Potential": {
        "type": "select",
        "options": ["Small <$50K", "Medium $50-200K", "Large $200K+", "Unknown"]
    },
    "Next Follow-Up": {"type": "date"},
    "Tags": {"type": "multi_select", "options": []},  # Dynamic
    
    # Metadata
    "Enrichment Date": {"type": "date"},
    "Data Quality Score": {"type": "number"},  # 1-10
    "Apollo ID": {"type": "rich_text"}
}
```

---

## API INTEGRATION SPECIFICATIONS

### 1. Apollo.io API Integration

```python
# enrichment/apollo_client.py

class ApolloClient:
    """
    Apollo.io API client with rate limiting and retry logic
    """
    
    BASE_URL = "https://api.apollo.io/v1"
    RATE_LIMIT = 60  # requests per minute
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.session = requests.Session()
        self.session.headers.update({
            "Content-Type": "application/json",
            "Cache-Control": "no-cache"
        })
    
    @tenacity.retry(
        wait=tenacity.wait_exponential(min=1, max=10),
        stop=tenacity.stop_after_attempt(3),
        retry=tenacity.retry_if_exception_type(requests.exceptions.RequestException)
    )
    @ratelimit.sleep_and_retry
    @ratelimit.limits(calls=RATE_LIMIT, period=60)
    def search_company(self, company_name: str) -> dict:
        """
        Search for company by name
        
        Returns:
        {
            'id': str,
            'name': str,
            'website_url': str,
            'linkedin_url': str,
            'industry': str,
            'employee_count': int,
            'estimated_revenue': str,
            'city': str,
            'state': str,
            'country': str,
            'technologies': [str],
            'funding_stage': str
        }
        """
        endpoint = f"{self.BASE_URL}/organizations/search"
        
        payload = {
            "api_key": self.api_key,
            "q_organization_name": company_name,
            "page": 1,
            "per_page": 1
        }
        
        response = self.session.post(endpoint, json=payload)
        response.raise_for_status()
        
        data = response.json()
        
        if data.get('organizations'):
            return self._normalize_company(data['organizations'][0])
        
        return None
    
    @ratelimit.sleep_and_retry
    @ratelimit.limits(calls=RATE_LIMIT, period=60)
    def search_people(
        self,
        company_id: str,
        titles: list[str],
        max_results: int = 10
    ) -> list[dict]:
        """
        Find decision makers at company
        
        Returns: [
            {
                'id': str,
                'name': str,
                'first_name': str,
                'last_name': str,
                'title': str,
                'email': str,  # or None if not revealed
                'phone': str,
                'linkedin_url': str,
                'seniority': str
            }
        ]
        """
        endpoint = f"{self.BASE_URL}/people/search"
        
        payload = {
            "api_key": self.api_key,
            "organization_ids": [company_id],
            "person_titles": titles,
            "page": 1,
            "per_page": max_results
        }
        
        response = self.session.post(endpoint, json=payload)
        response.raise_for_status()
        
        data = response.json()
        
        return [self._normalize_contact(p) for p in data.get('people', [])]
    
    @ratelimit.sleep_and_retry
    @ratelimit.limits(calls=RATE_LIMIT, period=60)
    def reveal_email(self, person_id: str) -> str:
        """
        Reveal email for a contact (uses 1 credit)
        
        Returns: email address or None
        """
        endpoint = f"{self.BASE_URL}/people/match"
        
        payload = {
            "api_key": self.api_key,
            "id": person_id,
            "reveal_personal_emails": True
        }
        
        response = self.session.post(endpoint, json=payload)
        response.raise_for_status()
        
        data = response.json()
        
        # Log credit usage
        self._log_credit_usage('email_reveal')
        
        return data.get('person', {}).get('email')
    
    def _normalize_company(self, raw_data: dict) -> dict:
        """Convert Apollo response to internal format"""
        # Implementation details...
        pass
    
    def _normalize_contact(self, raw_data: dict) -> dict:
        """Convert Apollo response to internal format"""
        # Implementation details...
        pass
    
    def _log_credit_usage(self, operation: str):
        """Log API credit usage to database"""
        # Implementation details...
        pass
```

### 2. Web Scraping Module

```python
# enrichment/web_scraper.py

class WebScraper:
    """
    Scrape company websites for descriptions
    """
    
    TIMEOUT = 5  # seconds
    USER_AGENT = "Mozilla/5.0 (compatible; OutreachCRM/1.0)"
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": self.USER_AGENT})
    
    def get_company_description(self, domain: str) -> str:
        """
        Scrape company description from About page
        
        Returns: description string (max 1000 chars) or None
        """
        about_urls = [
            f"https://{domain}/about",
            f"https://{domain}/about-us",
            f"https://{domain}/company",
            f"https://{domain}"  # fallback to homepage
        ]
        
        for url in about_urls:
            try:
                description = self._scrape_url(url)
                if description and len(description) > 100:
                    return description[:1000]
            except Exception as e:
                logger.warning(f"Failed to scrape {url}: {e}")
                continue
        
        return None
    
    def _scrape_url(self, url: str) -> str:
        """Scrape single URL for description"""
        response = self.session.get(url, timeout=self.TIMEOUT)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'lxml')
        
        # Remove script and style tags
        for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
            tag.decompose()
        
        # Try meta description first
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            return meta_desc['content'].strip()
        
        # Try og:description
        og_desc = soup.find('meta', attrs={'property': 'og:description'})
        if og_desc and og_desc.get('content'):
            return og_desc['content'].strip()
        
        # Find first substantial paragraph
        paragraphs = soup.find_all('p')
        for p in paragraphs:
            text = p.get_text(strip=True)
            if len(text) > 150:
                return text
        
        return None
```

### 3. News Scraper

```python
# enrichment/news_scraper.py

class NewsScraper:
    """
    Scrape recent news mentions from Google News
    """
    
    def get_recent_news(self, company_name: str, max_results: int = 5) -> list[dict]:
        """
        Get recent news articles
        
        Returns: [
            {
                'title': str,
                'source': str,
                'date': str,
                'url': str
            }
        ]
        """
        query = company_name.replace(' ', '+')
        url = f"https://news.google.com/search?q={query}&hl=en-US&gl=US&ceid=US:en"
        
        try:
            response = requests.get(url, timeout=5)
            soup = BeautifulSoup(response.text, 'lxml')
            
            articles = []
            for article in soup.find_all('article')[:max_results]:
                try:
                    title_elem = article.find('h3') or article.find('h4')
                    time_elem = article.find('time')
                    link_elem = article.find('a')
                    
                    if title_elem:
                        articles.append({
                            'title': title_elem.get_text(strip=True),
                            'date': time_elem['datetime'] if time_elem else None,
                            'url': f"https://news.google.com{link_elem['href']}" if link_elem else None,
                            'source': article.find('a').get_text(strip=True) if article.find('a') else None
                        })
                except Exception as e:
                    logger.warning(f"Failed to parse article: {e}")
                    continue
            
            return articles
            
        except Exception as e:
            logger.error(f"Failed to scrape news for {company_name}: {e}")
            return []
```

### 4. LinkedIn Finder (SerpAPI)

```python
# enrichment/linkedin_finder.py

class LinkedInFinder:
    """
    Find LinkedIn profile URLs using SerpAPI
    """
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.monthly_limit = 100
        self.used_searches = 0
    
    def find_profile(self, person_name: str, company_name: str) -> str:
        """
        Find LinkedIn profile URL
        
        Returns: LinkedIn URL or None
        """
        if self.used_searches >= self.monthly_limit:
            logger.warning("SerpAPI monthly limit reached")
            return None
        
        from serpapi import GoogleSearch
        
        query = f"{person_name} {company_name} site:linkedin.com/in/"
        
        params = {
            "q": query,
            "api_key": self.api_key,
            "num": 1
        }
        
        try:
            search = GoogleSearch(params)
            results = search.get_dict()
            
            self.used_searches += 1
            self._log_usage()
            
            if results.get('organic_results'):
                return results['organic_results'][0]['link']
            
        except Exception as e:
            logger.error(f"SerpAPI search failed: {e}")
        
        return None
    
    def _log_usage(self):
        """Log API usage to database"""
        # Implementation...
        pass
```

---

## CORE PROCESSING LOGIC

### Enrichment Engine (Main Orchestrator)

```python
# enrichment/enrichment_engine.py

class EnrichmentEngine:
    """
    Main orchestrator for company enrichment pipeline
    """
    
    def __init__(self, config):
        self.apollo = ApolloClient(config.APOLLO_API_KEY)
        self.scraper = WebScraper()
        self.news_scraper = NewsScraper()
        self.linkedin_finder = LinkedInFinder(config.SERPAPI_KEY)
        self.cache = CacheManager()
        self.logger = logging.getLogger(__name__)
    
    def enrich_company(self, company_name: str) -> dict:
        """
        Complete enrichment pipeline for one company
        
        Returns: {
            'company': {...},
            'contacts': [{...}],
            'status': 'success' | 'partial' | 'failed',
            'errors': [...]
        }
        """
        result = {
            'company': {},
            'contacts': [],
            'status': 'success',
            'errors': []
        }
        
        # Step 1: Check cache
        cached = self.cache.get_company(company_name)
        if cached and self._is_fresh(cached):
            self.logger.info(f"Using cached data for {company_name}")
            return cached
        
        # Step 2: Apollo company search
        try:
            apollo_company = self.apollo.search_company(company_name)
            if apollo_company:
                result['company'] = apollo_company
                self._log_step(company_name, 'apollo', 'success')
            else:
                self._log_step(company_name, 'apollo', 'failed', 'Company not found')
                result['errors'].append('Apollo: Company not found')
                result['status'] = 'partial'
        except Exception as e:
            self._log_step(company_name, 'apollo', 'failed', str(e))
            result['errors'].append(f'Apollo error: {e}')
            result['status'] = 'failed'
            return result
        
        # Step 3: Web scraping (enhance description)
        if result['company'].get('domain'):
            try:
                description = self.scraper.get_company_description(
                    result['company']['domain']
                )
                if description:
                    result['company']['description'] = description
                    self._log_step(company_name, 'scraping', 'success')
                else:
                    self._log_step(company_name, 'scraping', 'failed', 'No description found')
            except Exception as e:
                self._log_step(company_name, 'scraping', 'failed', str(e))
                result['errors'].append(f'Scraping error: {e}')
        
        # Step 4: Recent news
        try:
            news = self.news_scraper.get_recent_news(company_name)
            result['company']['recent_news'] = news
            self._log_step(company_name, 'news', 'success')
        except Exception as e:
            self._log_step(company_name, 'news', 'failed', str(e))
            result['errors'].append(f'News scraping error: {e}')
        
        # Step 5: Find decision makers
        if result['company'].get('apollo_id'):
            try:
                target_titles = self._get_target_titles(result['company'])
                contacts = self.apollo.search_people(
                    company_id=result['company']['apollo_id'],
                    titles=target_titles,
                    max_results=10
                )
                
                # Reveal emails for top 5 contacts only (save credits)
                for i, contact in enumerate(contacts[:5]):
                    if not contact.get('email') and contact.get('apollo_id'):
                        email = self.apollo.reveal_email(contact['apollo_id'])
                        contact['email'] = email
                
                result['contacts'] = contacts
                self._log_step(company_name, 'contacts', 'success')
                
            except Exception as e:
                self._log_step(company_name, 'contacts', 'failed', str(e))
                result['errors'].append(f'Contact search error: {e}')
                result['status'] = 'partial'
        
        # Step 6: Find LinkedIn URLs for contacts (optional, use sparingly)
        # Only for top 3 contacts, only if missing
        for contact in result['contacts'][:3]:
            if not contact.get('linkedin_url'):
                try:
                    linkedin_url = self.linkedin_finder.find_profile(
                        contact['name'],
                        company_name
                    )
                    contact['linkedin_url'] = linkedin_url
                except Exception as e:
                    self.logger.warning(f"LinkedIn search failed: {e}")
        
        # Step 7: Cache result
        self.cache.save_company(company_name, result)
        
        return result
    
    def _get_target_titles(self, company_data: dict) -> list[str]:
        """
        Determine which titles to search based on company type
        """
        industry = company_data.get('industry', '')
        
        # Base titles everyone gets
        base_titles = ['CEO', 'COO', 'CFO']
        
        # Add industry-specific titles
        if 'insurance' in industry.lower() or 'payer' in industry.lower():
            return base_titles + [
                'CMO', 'Chief Medical Officer',
                'VP Quality', 'VP Operations',
                'VP Medicare', 'VP Medicare Advantage',
                'Director Star Ratings', 'Director Quality'
            ]
        elif 'provider' in industry.lower() or 'health system' in industry.lower():
            return base_titles + [
                'VP Operations', 'VP Care Management',
                'VP Population Health', 'Chief Clinical Officer',
                'Director Care Management'
            ]
        elif 'pharmacy' in industry.lower() or 'pbm' in industry.lower():
            return base_titles + [
                'VP Pharmacy Operations', 'VP Clinical Programs',
                'Director Adherence', 'Chief Pharmacy Officer'
            ]
        else:
            return base_titles + ['CTO', 'VP Innovation', 'VP Strategy']
    
    def _is_fresh(self, cached_data: dict, max_age_days: int = 7) -> bool:
        """Check if cached data is still fresh"""
        # Implementation...
        pass
    
    def _log_step(self, company_name: str, step: str, status: str, error: str = None):
        """Log enrichment step to database"""
        # Implementation...
        pass
```

### Tier Assignment Logic

```python
# processors/tier_assigner.py

class TierAssigner:
    """
    Auto-assign tier based on company characteristics
    """
    
    TIER_1_KEYWORDS = ['imo', 'agent', 'broker', 'medicare advisor']
    TIER_2_KEYWORDS = ['medicare advantage', 'health plan', 'insurance', 'payer']
    TIER_3_KEYWORDS = ['aco', 'mso', 'provider', 'medical group', 'health system']
    TIER_4_KEYWORDS = ['pharma', 'pharmaceutical', 'biotech', 'drug']
    
    def assign_tier(self, company_data: dict) -> str:
        """
        Assign tier based on industry and keywords
        
        Returns: "Tier 1 - AEP Urgent" | "Tier 2 - Strategic" | etc.
        """
        industry = company_data.get('industry', '').lower()
        name = company_data.get('name', '').lower()
        description = company_data.get('description', '').lower()
        
        search_text = f"{industry} {name} {description}"
        
        # Check Tier 1 (AEP Urgent)
        if any(kw in search_text for kw in self.TIER_1_KEYWORDS):
            return "Tier 1 - AEP Urgent"
        
        # Check Tier 2 (Strategic - Health Plans)
        if any(kw in search_text for kw in self.TIER_2_KEYWORDS):
            return "Tier 2 - Strategic"
        
        # Check Tier 3 (Providers)
        if any(kw in search_text for kw in self.TIER_3_KEYWORDS):
            return "Tier 3 - Proven Vertical"
        
        # Check Tier 4 (Pharma)
        if any(kw in search_text for kw in self.TIER_4_KEYWORDS):
            return "Tier 4 - Exploratory"
        
        # Default to Tier 3
        return "Tier 3 - Proven Vertical"
```

### Priority Scoring

```python
# processors/priority_scorer.py

class PriorityScorer:
    """
    Calculate priority score (1-10) for each company
    """
    
    def calculate_priority(self, company_data: dict, contacts: list) -> int:
        """
        Score based on multiple factors
        
        Scoring factors:
        - Company size (larger = higher)
        - Revenue (higher = higher)
        - Contact quality (verified emails = higher)
        - Recent news (more recent = higher)
        - Tech stack fit (relevant tech = higher)
        """
        score = 5  # Base score
        
        # Factor 1: Company size (+0 to +2)
        size = company_data.get('employee_count', 0)
        if size > 5000:
            score += 2
        elif size > 1000:
            score += 1.5
        elif size > 200:
            score += 1
        elif size < 50:
            score -= 0.5  # Too small might not be priority
        
        # Factor 2: Revenue (+0 to +2)
        revenue = company_data.get('revenue_range', '')
        if '$200M+' in revenue:
            score += 2
        elif '$50-200M' in revenue:
            score += 1.5
        elif '$10-50M' in revenue:
            score += 1
        
        # Factor 3: Contact quality (+0 to +2)
        emails_found = sum(1 for c in contacts if c.get('email'))
        if emails_found >= 3:
            score += 2
        elif emails_found >= 2:
            score += 1
        elif emails_found == 1:
            score += 0.5
        
        # Factor 4: Recent news (+0 to +1)
        news = company_data.get('recent_news', [])
        if len(news) > 3:
            score += 1
        elif len(news) > 0:
            score += 0.5
        
        # Factor 5: Tech stack fit (+0 to +1)
        tech_stack = company_data.get('tech_stack', [])
        relevant_tech = ['salesforce', 'epic', 'cerner', 'aws', 'azure']
        if any(tech.lower() in [t.lower() for t in tech_stack] for tech in relevant_tech):
            score += 1
        
        # Factor 6: Tier boost
        tier = company_data.get('tier', '')
        if 'Tier 1' in tier:
            score += 2  # AEP urgent
        elif 'Tier 2' in tier:
            score += 1  # Strategic
        
        # Clamp to 1-10
        return max(1, min(10, int(score)))
```

---

## NOTION INTEGRATION

```python
# integrations/notion_client.py

class NotionClient:
    """
    Notion API wrapper for CRM operations
    """
    
    def __init__(self, token: str, database_id: str):
        from notion_client import Client
        self.client = Client(auth=token)
        self.database_id = database_id
    
    def create_company_page(
        self,
        company_data: dict,
        contacts: list,
        tier: str,
        priority: int
    ) -> str:
        """
        Create new page in Notion database
        
        Returns: notion_page_id
        """
        properties = self._build_properties(company_data, contacts, tier, priority)
        
        response = self.client.pages.create(
            parent={"database_id": self.database_id},
            properties=properties
        )
        
        return response['id']
    
    def _build_properties(
        self,
        company_data: dict,
        contacts: list,
        tier: str,
        priority: int
    ) -> dict:
        """
        Build Notion properties object
        """
        # Get top 3 contacts
        top_contacts = sorted(
            contacts,
            key=lambda c: c.get('priority_score', 0),
            reverse=True
        )[:3]
        
        properties = {
            "Company Name": {
                "title": [{"text": {"content": company_data.get('name', '')}}]
            },
            "Status": {
                "select": {"name": "Not Contacted"}
            },
            "Industry": {
                "select": {"name": self._map_industry(company_data.get('industry'))}
            },
            "Tier": {
                "select": {"name": tier}
            },
            "Priority Score": {
                "number": priority
            },
            "Company Website": {
                "url": company_data.get('domain')
            },
            "Company LinkedIn": {
                "url": company_data.get('linkedin_url')
            },
            "Company Description": {
                "rich_text": [{"text": {"content": company_data.get('description', '')[:2000]}}]
            },
            "Company Size": {
                "select": {"name": self._map_size(company_data.get('employee_count'))}
            },
            "Location": {
                "rich_text": [{"text": {"content": company_data.get('location', '')}}]
            },
            "Revenue Range": {
                "select": {"name": company_data.get('revenue_range', 'Unknown')}
            },
            "Funding Stage": {
                "select": {"name": company_data.get('funding_stage', 'Unknown')}
            },
            "Recent News": {
                "rich_text": [{"text": {"content": self._format_news(company_data.get('recent_news', []))}}]
            },
            "Enrichment Date": {
                "date": {"start": datetime.now().isoformat()}
            },
            "Data Quality Score": {
                "number": self._calculate_data_quality(company_data, contacts)
            },
            "Apollo ID": {
                "rich_text": [{"text": {"content": company_data.get('apollo_id', '')}}]
            }
        }
        
        # Add top contacts
        if len(top_contacts) > 0:
            properties["Primary Contact Name"] = {
                "rich_text": [{"text": {"content": top_contacts[0].get('name', '')}}]
            }
            properties["Primary Contact Title"] = {
                "rich_text": [{"text": {"content": top_contacts[0].get('title', '')}}]
            }
            if top_contacts[0].get('email'):
                properties["Primary Contact Email"] = {
                    "email": top_contacts[0]['email']
                }
            if top_contacts[0].get('linkedin_url'):
                properties["Primary Contact LinkedIn"] = {
                    "url": top_contacts[0]['linkedin_url']
                }
        
        # Similar for secondary and tertiary contacts...
        
        return properties
    
    def _map_industry(self, industry: str) -> str:
        """Map Apollo industry to Notion options"""
        industry_lower = (industry or '').lower()
        
        if any(kw in industry_lower for kw in ['insurance', 'payer', 'health plan']):
            return "Insurance / Payer"
        elif any(kw in industry_lower for kw in ['hospital', 'health system', 'provider', 'clinic']):
            return "Healthcare Provider / Health System"
        elif any(kw in industry_lower for kw in ['pharmacy', 'pbm']):
            return "Pharmacy / PBM"
        elif any(kw in industry_lower for kw in ['pharma', 'biotech', 'drug']):
            return "Pharma / Biotech"
        else:
            return "Other"
    
    def _map_size(self, employee_count: int) -> str:
        """Map employee count to size range"""
        if not employee_count:
            return "Unknown"
        elif employee_count < 50:
            return "1-50"
        elif employee_count < 200:
            return "51-200"
        elif employee_count < 1000:
            return "201-1000"
        elif employee_count < 5000:
            return "1000-5000"
        else:
            return "5000+"
    
    def _format_news(self, news_list: list) -> str:
        """Format news array into readable string"""
        if not news_list:
            return ""
        
        formatted = []
        for article in news_list[:5]:
            formatted.append(
                f"• {article.get('title', 'No title')} ({article.get('date', 'No date')})\n"
                f"  {article.get('url', '')}"
            )
        
        return "\n\n".join(formatted)[:2000]  # Notion limit
    
    def _calculate_data_quality(self, company_data: dict, contacts: list) -> int:
        """
        Calculate data quality score (1-10)
        Based on completeness of enriched data
        """
        score = 0
        
        # Check company data completeness
        if company_data.get('domain'): score += 1
        if company_data.get('description'): score += 1
        if company_data.get('industry'): score += 1
        if company_data.get('employee_count'): score += 1
        if company_data.get('revenue_range'): score += 1
        if company_data.get('linkedin_url'): score += 1
        if company_data.get('recent_news'): score += 1
        
        # Check contact quality
        emails_found = sum(1 for c in contacts if c.get('email'))
        if emails_found >= 3: score += 3
        elif emails_found >= 2: score += 2
        elif emails_found >= 1: score += 1
        
        return min(10, score)
    
    def page_exists(self, company_name: str) -> bool:
        """Check if company already exists in Notion"""
        response = self.client.databases.query(
            database_id=self.database_id,
            filter={
                "property": "Company Name",
                "title": {
                    "equals": company_name
                }
            }
        )
        
        return len(response['results']) > 0
```

---

## CLI INTERFACE

```python
# main.py

import click
from rich.console import Console
from rich.progress import track
from rich.table import Table

console = Console()

@click.group()
def cli():
    """HLTH Outreach CRM - Company Enrichment Tool"""
    pass

@cli.command()
@click.argument('csv_file', type=click.Path(exists=True))
@click.option('--batch-size', default=10, help='Process N companies at a time')
@click.option('--skip-duplicates/--no-skip-duplicates', default=True)
def enrich(csv_file, batch_size, skip_duplicates):
    """
    Enrich companies from CSV and sync to Notion
    
    CSV Format:
    company_name
    Acme Health
    Beta Medical
    ...
    """
    console.print(f"[bold blue]Starting enrichment from {csv_file}[/bold blue]\n")
    
    # Load CSV
    df = pd.read_csv(csv_file)
    companies = df['company_name'].tolist()
    
    console.print(f"Found {len(companies)} companies to enrich\n")
    
    # Initialize engine
    config = load_config()
    engine = EnrichmentEngine(config)
    notion = NotionClient(config.NOTION_TOKEN, config.NOTION_DB_ID)
    tier_assigner = TierAssigner()
    priority_scorer = PriorityScorer()
    
    # Process in batches
    results = {
        'success': 0,
        'partial': 0,
        'failed': 0,
        'skipped': 0
    }
    
    for company_name in track(companies, description="Enriching..."):
        try:
            # Check if exists
            if skip_duplicates and notion.page_exists(company_name):
                console.print(f"⏭️  Skipped (already exists): {company_name}")
                results['skipped'] += 1
                continue
            
            # Enrich
            enriched = engine.enrich_company(company_name)
            
            if enriched['status'] == 'failed':
                console.print(f"❌ Failed: {company_name}")
                results['failed'] += 1
                continue
            
            # Assign tier and priority
            tier = tier_assigner.assign_tier(enriched['company'])
            priority = priority_scorer.calculate_priority(
                enriched['company'],
                enriched['contacts']
            )
            
            enriched['company']['tier'] = tier
            enriched['company']['priority'] = priority
            
            # Sync to Notion
            page_id = notion.create_company_page(
                enriched['company'],
                enriched['contacts'],
                tier,
                priority
            )
            
            # Status
            if enriched['status'] == 'success':
                console.print(f"✅ Success: {company_name} (Priority: {priority})")
                results['success'] += 1
            else:
                console.print(f"⚠️  Partial: {company_name} (Priority: {priority})")
                results['partial'] += 1
            
            # Rate limiting
            time.sleep(2)  # Be nice to APIs
            
        except Exception as e:
            console.print(f"❌ Error: {company_name} - {str(e)}")
            results['failed'] += 1
    
    # Summary
    console.print("\n[bold green]Enrichment Complete![/bold green]\n")
    
    table = Table(title="Results Summary")
    table.add_column("Status", style="cyan")
    table.add_column("Count", style="magenta")
    
    table.add_row("✅ Success", str(results['success']))
    table.add_row("⚠️ Partial", str(results['partial']))
    table.add_row("❌ Failed", str(results['failed']))
    table.add_row("⏭️ Skipped", str(results['skipped']))
    
    console.print(table)

@cli.command()
def check_credits():
    """Check remaining API credits"""
    config = load_config()
    
    # Query usage from database
    # Display current usage vs limits
    # Implementation...
    pass

@cli.command()
@click.argument('company_name')
def test_enrich(company_name):
    """Test enrichment on a single company"""
    console.print(f"[bold]Testing enrichment for: {company_name}[/bold]\n")
    
    config = load_config()
    engine = EnrichmentEngine(config)
    
    result = engine.enrich_company(company_name)
    
    # Pretty print results
    console.print(result)

if __name__ == '__main__':
    cli()
```

---

## CONFIGURATION

```python
# config/settings.py

import os
from dotenv import load_dotenv

load_dotenv()

class Config:
    # API Keys
    APOLLO_API_KEY = os.getenv('APOLLO_API_KEY')
    SERPAPI_KEY = os.getenv('SERPAPI_KEY')
    NOTION_TOKEN = os.getenv('NOTION_TOKEN')
    NOTION_DB_ID = os.getenv('NOTION_DB_ID')
    
    # Rate Limits
    APOLLO_RATE_LIMIT = 60  # per minute
    SERPAPI_RATE_LIMIT = 100  # per month
    
    # Database
    SQLITE_DB_PATH = 'database/cache.db'
    
    # Scraping
    REQUEST_TIMEOUT = 5  # seconds
    MAX_RETRIES = 3
    
    # Cache
    CACHE_EXPIRY_DAYS = 7
    
    # Enrichment
    MAX_CONTACTS_PER_COMPANY = 10
    EMAIL_REVEAL_LIMIT = 5  # Only reveal top 5 contacts
    
    @classmethod
    def validate(cls):
        """Validate required config"""
        required = [
            'APOLLO_API_KEY',
            'NOTION_TOKEN',
            'NOTION_DB_ID'
        ]
        
        missing = [key for key in required if not getattr(cls, key)]
        
        if missing:
            raise ValueError(f"Missing required config: {', '.join(missing)}")
```

```.env file
# .env

APOLLO_API_KEY=your_apollo_key_here
SERPAPI_KEY=your_serpapi_key_here
NOTION_TOKEN=your_notion_integration_token
NOTION_DB_ID=your_notion_database_id
```

---

## IMPLEMENTATION STEPS FOR ENGINEER

### Day 1: Setup & Foundation
1. Create project structure
2. Install dependencies (`pip install -r requirements.txt`)
3. Setup .env file with API keys
4. Create Notion database with properties
5. Initialize SQLite database with schema
6. Test Apollo API connection
7. Test Notion API connection

### Day 2: Core Enrichment
1. Implement `apollo_client.py`
2. Implement `web_scraper.py`
3. Implement `news_scraper.py`
4. Implement `enrichment_engine.py`
5. Test enrichment on 3 companies
6. Fix any API issues

### Day 3: Processing & Notion Sync
1. Implement `tier_assigner.py`
2. Implement `priority_scorer.py`
3. Implement `notion_client.py`
4. Implement `cache_manager.py`
5. Test full pipeline on 5 companies

### Day 4: CLI & Polish
1. Implement `main.py` CLI interface
2. Add logging
3. Add error handling
4. Add progress bars
5. Test on 20 companies
6. Document usage

### Day 5: Testing & Launch
1. Process full company list
2. Verify Notion data quality
3. Fix any issues
4. Document API usage
5. Ready for production use

---

## ERROR HANDLING STRATEGY

```python
# utils/error_handler.py

class EnrichmentError(Exception):
    """Base exception for enrichment errors"""
    pass

class APILimitError(EnrichmentError):
    """Raised when API limit exceeded"""
    pass

class DataQualityError(EnrichmentError):
    """Raised when data quality is too low"""
    pass

def handle_enrichment_error(func):
    """Decorator for error handling"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except requests.exceptions.Timeout:
            logger.error(f"Timeout in {func.__name__}")
            return None
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                raise APILimitError("Rate limit exceeded")
            logger.error(f"HTTP error in {func.__name__}: {e}")
            return None
        except Exception as e:
            logger.exception(f"Unexpected error in {func.__name__}")
            return None
    return wrapper
```

---

## TESTING CHECKLIST

```python
# tests/test_integration.py

def test_full_pipeline():
    """Test complete enrichment pipeline"""
    
    test_companies = [
        "Bright Health",
        "ChenMed",
        "Alto Pharmacy"
    ]
    
    for company in test_companies:
        # Test enrichment
        result = engine.enrich_company(company)
        
        assert result is not None
        assert result['company']['name'] == company
        assert len(result['contacts']) > 0
        assert result['status'] in ['success', 'partial']
        
        # Test Notion sync
        page_id = notion.create_company_page(...)
        assert page_id is not None
        
        # Verify in Notion
        assert notion.page_exists(company)
```

---

## USAGE EXAMPLES

```bash
# Setup
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Configure
cp .env.example .env
# Edit .env with your API keys

# Test single company
python main.py test-enrich "Bright Health"

# Enrich from CSV
python main.py enrich companies.csv

# Enrich with options
python main.py enrich companies.csv --batch-size 5 --skip-duplicates

# Check API credit usage
python main.py check-credits
```

---

## SUCCESS METRICS

After implementation, you should achieve:

- ✅ **90%+ enrichment success rate**
- ✅ **5-10 seconds per company** (with caching)
- ✅ **Automated tier assignment** (95%+ accuracy)
- ✅ **3-5 verified contacts** per company
- ✅ **Full Notion sync** with all fields populated
- ✅ **Zero manual data entry** required

---

**This architecture is production-ready and can be implemented in 3-5 days by a mid-level engineer.**

Want me to start coding any specific module?